{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline + Late Penalty\n",
    "\n",
    "$\\textbf{Note:}$ It will take you quite some time to complete this project, therefore, we earnestly recommend that you start working as early as possible. You should read the specs carefully at least 2-3 times before you start coding.\n",
    "\n",
    "* $\\textbf{Submission deadline for the Project (Part-1) is 20:59:59 (08:59:59 PM) on 4th Nov, 2019}$\n",
    "* $\\textbf{LATE PENALTY: 10% on day-1 and 20% on each subsequent day.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "1. This note book contains instructions for $\\textbf{COMP6714-Project (Part-1)}$. We will release the instructions for the $\\textbf{Part-2 of the Project}$ in a seperate notebook. \n",
    "\n",
    "* You are required to complete your implementation for part-1 in a file `project_part1.py` provided along with this notebook. Please $\\textbf{DO NOT ALTER}$ the name of the file.\n",
    "\n",
    "* You are not allowed to print out unnecessary stuff. We will not consider any output printed out on the screen. All results should be returned in appropriate data structures via corresponding functions.\n",
    "\n",
    "* You can submit your implementation for **Project (Part-1)** via submission system: http://kg.cse.unsw.edu.au/submit/ . We have already sent out the invitations for you to join the submission system. In case of problems please post your request @ Piazza.\n",
    "\n",
    "* For each question, we have provided you with detailed instructions along with question headings. In case of problems, you can post your query @ Piazza.\n",
    "\n",
    "* You are allowed to add other functions and/or import modules (you may have to for this project), but you are not allowed to define global variables. **Only functions are allowed** in `project_part1.py`\n",
    "\n",
    "* You should not import unnecessary and non-standard modules/libraries. Loading such libraries at test time will lead to errors and hence 0 mark for your project. If you are not sure, please ask @ Piazza. \n",
    "\n",
    "* We will provide immediate feedback on your submission. You can access your scores using the online submission portal on the same day. \n",
    "\n",
    "* For the **Final Evaluation**, we will be using a different dataset, so your final scores may vary.  \n",
    "\n",
    "* You are allowed to have a limited number of Feedback Attempts $\\textbf{(15 Attempts for each student)}$, we will use your **LAST** submission for Final Evaluation.\n",
    "\n",
    "### Allowed Libraries:\n",
    "\n",
    "You are required to write your implementation for the project (part-1) using `Python 3.6.5`. You are only allowed to use the following python libraries:\n",
    "* $\\textbf{spacy (v2.1.8)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Compute TF-IDF score for query (80 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this project, you are required to compute $TF\\text{-}IDF$ score of a document $D_{j}$ $\\textit{w.r.t}$ an input query $Q$ and a Dictionary of Entities $(DoE)$.\n",
    "\n",
    "### Inputs (Q1):\n",
    "Inputs to your model are as follows:\n",
    "1. Documents ($D$) as a dictionary with $key:$ doc_id; $value:$ document text\n",
    "* Query ($Q$), as a string of words\n",
    "* Dictionary of Entities ($DoE$), with $key:$ entity; $value:$ entity_id\n",
    "\n",
    "The procedure for computation of the $TF\\text{-}IDF$ score follows following steps:\n",
    "\n",
    "1. $\\textbf{TF-IDF index construction for Entities and Tokens}$\n",
    "* $\\textbf{Split the query into lists of Entities and Tokens}$\n",
    "* $\\textbf{Query Score Computation}$\n",
    "\n",
    "Detailed description of these steps is as under:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TF-IDF index construction for Entities and Tokens\n",
    "\n",
    "We require you to separately learn TF-IDF index for tokens ($TF\\text{-}IDF_{token}$) and entities ($TF\\text{-}IDF_{entity}$). The computation of each of the TF-IDF index is given as follows: \n",
    "\n",
    "### TF-IDF index For Tokens:\n",
    "\n",
    "The term frequency of the token $t$ in a document $D_{j}$ is computed as follows:\n",
    "\n",
    "$$\n",
    "TF_{token}(t,D_{j}) = {\\# \\; of \\; times \\; token \\; t \\; appears \\; in \\; D_{j}}\n",
    "$$\n",
    "\n",
    "\n",
    "To de-emphasize the high token frequency, we apply double log to normalize the term frequency. The computation of normalized term frequency of token $t$ is illustrated as follows:\n",
    "\n",
    "$$\n",
    "TF_{norm\\_token}(t,D_{j}) =  1.0 + \\ln(1.0 + \\ln(TF_{token}(t,D_{j})))\n",
    "$$\n",
    "\n",
    "And, the Inverse Document Frequency of the token $t$ is computed as follows: \n",
    "\n",
    "$$\n",
    "IDF_{token}(t) = 1.0 + \\ln(\\frac{total \\; \\# \\; of \\; docs}{1.0 + \\# \\; of \\; docs \\; containing \\; token \\; \\textit{t}})\n",
    "$$\n",
    "\n",
    "The TF-IDF score of token $t$ in document $D_{j}$ is computed as: <br>\n",
    "\n",
    "$$\n",
    "TF\\text{-}IDF_{token}(t,D_{j}) = TF_{norm\\_token}(t,D_{j}) * IDF_{token}(t)\n",
    "$$\n",
    "\n",
    "\n",
    "### TF-IDF index for Entities:\n",
    "The term frequency of the entity $e$ in a document $D_{j}$ is computed as follows:\n",
    "\n",
    "$$\n",
    "TF_{entity}(e,D_{j}) = {\\# \\; of \\; times \\; entity \\; e \\; appears \\; in \\; D_{j}}\n",
    "$$\n",
    "\n",
    "We simply use natural log to normalize the term frequency of the entities, as given below:\n",
    "\n",
    "$$\n",
    "TF_{norm\\_entity}(e,D_{j}) =  1.0 + \\ln(TF_{entity}(e,D_{j}))\n",
    "$$\n",
    "\n",
    "And, the Inverse Document Frequency of the entity $e$ is computed as follows: \n",
    "\n",
    "\n",
    "$$\n",
    "IDF_{entity}(e) = 1.0 + \\ln(\\frac{total \\; \\# \\; of \\; docs}{1.0 + \\# \\; of \\; docs \\; containing \\; entity \\; \\textit{e}})\n",
    "$$\n",
    "\n",
    "\n",
    "The TF-IDF score of the entity $e$ in the document $D_{j}$ is computed as: <br>\n",
    "\n",
    "$$\n",
    "TF\\text{-}IDF_{entity}(e,D_{j}) = TF_{norm\\_entity}(e,D_{j}) * IDF_{entity}(e)\n",
    "$$\n",
    "\n",
    "$\\textbf{Note:}$ We assign `TF-IDF score = 0.0` for the cases where the term frequency (TF) for the token and/or entity is `ZERO`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split the Query into Entities and Tokens:\n",
    "\n",
    "At first, you are required to split the query ($Q$) into all possible combinations of free keywords, i.e., tokens ($K = \\{k_{i}\\}_{i=1}^{N}$) and entities ($E= \\{e_{i}\\}_{i=1}^{N}$), where entities correspond to a subset of entities found in $DoE$ formed by individual and/or combination of tokens in $Q$. This process is explained below:\n",
    "\n",
    "> $\\textbf{Step 1:}$ We look for probable entities in the $Q$ by considering individual and/or combination of query tokens formed by combining the tokens in the increasing order of the query string. Amonst them, we only select the entities present in $DoE$.<br>\n",
    "> $\\textbf{Step 2:}$ Based on the selected list of entities found in $\\textbf{Step-1}$ enumerate all possible subsets of entities.<br>\n",
    "> $\\textbf{Step 3:}$ Filter subsets of entities found in $\\textbf{Step-2}$ such that for each subset the token count does not exceed the corresponding token count in $Q$. We treat the filtered subset as the final entities of the corresponding query split.<br>\n",
    "> $\\textbf{Step 4:}$ For each filtered entity subset, the rest of the keywords in the query, i.e., $(Q \\setminus wordsInEntities(e_{i}))$ are treated as the tokens of the query split.<br>\n",
    "\n",
    "\n",
    "Formally, let query be a a string of tokens, e.g., $Q = \\;\"A\\;B \\;C \\;D \\;E \\;F\\; G\"$ and dictionary of entities be $DoE = \\{AB, DF, GK\\}$. The list of entities formed by the tokens in the query and/or combinations of query tokens (contained in $DoE$) is $[AB, DF]$ and upon enumerating the possible subsets of the entities, we get following different possible splits of the query to the lists of the entities and the tokens:\n",
    "\n",
    "$\\textbf{Split-1:}$ $e_{1} = []$; $k_{1} = [A,B,C,D,E,F,G]$\n",
    "\n",
    "$\\textbf{Split-2:}$ $e_{2} = [AB]$; $k_{2} = [C,D,E,F,G]$\n",
    "\n",
    "$\\textbf{Split-3:}$ $e_{3} = [DF]$; $k_{3} = [A,B,C,E,G]$\n",
    "\n",
    "$\\textbf{Split-4:}$ $e_{4} = [AB, DF ]$; $k_{4} = [C,E,G]$\n",
    "\n",
    "$\\textbf{Note:}$ <br>\n",
    "1. In order to split the query, we only care about the subset of entities contained in $DoE$ that can be formed by individual and/or combination of tokens in the $Q$.\n",
    "\n",
    "* Entities in $DoE$ may correspond to single and/or multiple tokens, e.g., in the example given above $A$, $ABC$ etc., may also correspond to valid entities and may appear in the $DoE$.\n",
    "\n",
    "* Maximum number of query splits are upper-bounded by the subset of the entities in $DoE$ that can be formed by the tokens in the $Q$.\n",
    "\n",
    "* For every query split, the leftover keywords $Q \\setminus wordsInEntities(e_{i})$ are considered as the corresponding token split.\n",
    "\n",
    "* In order to form entities, we only combine keywords in the increasing order of the elements in the query string. For example, in $Q =\\; \"A\\; B\\; C\\; D\\; E\\; F\\; G\\;\"$, the entities such as: $BA$, $CB$ etc., will not be considered as entities and hence will not appear in the $DoE$.\n",
    "\n",
    "* In the example given above, if $DoE$ = $\\{AB, BC\\}$, then there will be only three possible splits of the query. Because the $Q$ contains only one instance of the token $B$, so it will not be possible to form a subset with multiple entities $[AB, BC]$, as it would require at least two instances of token $B$ in the $Q$ (also discussed in $\\textbf{Step-3}$ above )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query Score Computation:\n",
    "\n",
    "Later, you are required to use the corresponding $TF\\text{-}IDF$ index to separately compute scores for the list of tokens and entities corresponding to each query split, i.e., $(k_{i},e_{i})$, $\\textit{w.r.t}$ the document $D_{j}$ as follows:\n",
    "\n",
    "\n",
    "$$\n",
    "s_{i1} = \\sum_{entity \\in e_{i}} TF_{norm\\_entity}(entity,D_{j}) * IDF_{entity}(entity) \\\\\n",
    "s_{i2} = \\sum_{token \\in k_{i}} TF_{norm\\_token}(token,D_{j}) * IDF_{token}(token) \\\\\n",
    "score_{i}(\\{k_{i},e_{i}\\}, D_{j}) = s_{i1} + \\lambda * s_{i2}|_{\\lambda = 0.4}\n",
    "$$\n",
    "\n",
    "Finally, you are required to return the maximum score among all the query splits, i.e.,\n",
    "\n",
    "$$\n",
    "score_{max} = max\\{score_{i}\\}_{i=1}^{N}\\\\\n",
    "$$\n",
    "\n",
    "Note, in the above-mentioned equations, we use two separate $TF\\text{-}IDF$ indexes, i.e., ($TF\\text{-}IDF_{token}$) and ($TF\\text{-}IDF_{entity}$) to compute the scores for the token splits and the entity splits of the query respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some key instructions regarding TF-IDF indexing, parsing and text processing are as follows:\n",
    "\n",
    "### Key Instructions:\n",
    "\n",
    "1. **Note** that for a given set of documents, you only need to index the documents only once and later use your index to compute the query scores.\n",
    "\n",
    "* You are only allowed to use Spacy (v2.1.8) for text processing and parsing. You can install the Spacy via following web-link: [Spacy](https://spacy.io/usage)\n",
    "\n",
    "* We assume the parsing result of Spacy is always correct, we will not cater to any in-consistency in the Spacy's parsing results. \n",
    "\n",
    "* All the tokens in the documents $(D)$, query $(Q)$ and dictionary of entities $(DoE)$ are case-sensitive. You  $\\textbf{SHOULD NOT ALTER}$ the case of the tokens.\n",
    "\n",
    "* You are required to compute two separate indexes, i.e., (i) For tokens, and (ii) For Entities, such that:\n",
    "\n",
    "> 1. In order to compute the index of the Entities (i.e., $TF\\text{-}IDF_{entity}$), you should index all the entities detected by spacy irrespective of their entity types and/or presence in $DoE$. For details on spacy's parsing and entity recognition, please see the web-link: [Spacy Parsing](https://spacy.io/usage/linguistic-features)<br>\n",
    "> 2. For single-word Entities, e.g., `Trump` etc., you should only compute the index corresponding to the entities. For such entities, you should not consider the corresponding token for computing the TF-IDF index of tokens.<br>\n",
    "> 3. For multi-word entities, e.g., `New York Times` etc., individual tokens corresponding to the entities should be considered as free tokens and should be indexed while TF-IDF index construction of tokens (i.e., $TF\\text{-}IDF_{token}$).<br>\n",
    "\n",
    "* `Stopwords`: You should only use the token's attribute `is_stop` on a string parsed by Spacy to declare any token as stopword and eventually remove it. This also applies to stopwords within multi-word entities, e.g., `Times of India`.\n",
    "\n",
    "* `Punctuation`: You should only use the token's attribute `is_punct` on a string parsed by Spacy to decalre any token as a punctuation mark and eventually remove it.\n",
    "\n",
    "* `Special Cases`: You should not explicitly strip out punctuations or amend the Spacy's tokenization and parsing results. Some examples in this regard are as follows:\n",
    "> 1. In the sentence: `I am going to U.S.` the correctly extracted entity is `U.S.`<br>\n",
    "  2. Likewise, in the sentence: `I am going to school.` the spacy will extract the token `school` and will consider the fullstop `.`  as a punctuation mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example for Illustration\n",
    "\n",
    "Here, we provide a small toy example for illustration: <br>\n",
    "Let the dictionary of documents ($D$) be:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "documents = {1:'President Trump was on his way to new New York in New York City.',\n",
    "             2:'New York Times mentioned an interesting story about Trump.',\n",
    "             3:'I think it would be great if I can travel to New York this summer to see Trump.'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term frequencies corresponding to the tokens (i.e., $TF_{token}$) are shown below as a dictionary of dictionary of the form: <br> \n",
    "$\\{token$ : $\\{doc\\_id: count\\}\\}$."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{'President': {1: 1},\n",
    " 'way': {1: 1},\n",
    " 'new': {1: 1},\n",
    " 'New': {1: 2, 2: 1, 3: 1},\n",
    " 'York': {1: 2, 2: 1, 3: 1},\n",
    " 'City': {1: 1},\n",
    " 'Times': {2: 1},\n",
    " 'mentioned': {2: 1},\n",
    " 'interesting': {2: 1},\n",
    " 'story': {2: 1},\n",
    " 'think': {3: 1},\n",
    " 'great': {3: 1},\n",
    " 'travel': {3: 1},\n",
    " 'summer': {3: 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, The term frequencies corresponding to the entities (i.e., $TF_{entity}$) are shown below as a dictionary of dictionary of the form: <br> \n",
    "$\\{entity$ : $\\{doc\\_id: count\\}\\}$."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{'Trump': {1: 1, 2: 1, 3: 1},\n",
    " 'New York': {1: 1, 3: 1},\n",
    " 'New York City': {1: 1},\n",
    " 'New York Times': {2: 1},\n",
    " 'this summer': {3: 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the query ($Q$) be:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q = 'New York Times Trump travel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the $DoE$ be:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DoE = {'New York Times':0, 'New York':1,'New York City':2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The possible query splits are:\n",
    "\n",
    "$e_1$ = [], $k_1$ =  [`New`, `York`, `Times`, `Trump`, `travel`]\n",
    "\n",
    "$e_2$ = [`New York Times`], $k_2$ = [`Trump`, `travel`]\n",
    "\n",
    "$e_3$ = [`New York`], $k_3$= [`Times`, `Trump`, `travel`]\n",
    "\n",
    "$\\textbf{Note:}$ We cannot select the query split with the entity part as the combination of following entities: $e_{i}$ = [`New York`, `New York Times`], because there are only single instances of the tokens `New` and `York` in the $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `doc_id=3`, after applying the formulas mentioned in sub-headings `2,3` given above, we get following scores for all the query splits:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query =  {'tokens': ['New', 'York', 'Times', 'Trump', 'travel'], 'entities': []}\n",
    "{'tokens_score': 2.8301009632046026, 'entities_score': 0.0, 'combined_score': 1.132040385281841}\n",
    "\n",
    "query =  {'tokens': ['Trump', 'travel'], 'entities': ['New York Times']}\n",
    "{'tokens_score': 1.4054651081081644, 'entities_score': 0.0, 'combined_score': 0.5621860432432658}\n",
    "\n",
    "query =  {'tokens': ['Times', 'Trump', 'travel'], 'entities': ['New York']}\n",
    "{'tokens_score': 1.4054651081081644, 'entities_score': 1.0, 'combined_score': 1.562186043243266}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the maximum score `max_score` among all the query splits is: <br>\n",
    "\n",
    "`1.562186043243266` <br>\n",
    "\n",
    "And, the corresponding query split is:<br>\n",
    "\n",
    "`{'tokens': ['Times', 'Trump', 'travel'], 'entities': ['New York']}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Format (Q1):\n",
    "Your output should be a tuple of the form:<br> \n",
    "`(max_score, {'tokens':[...], 'entities':[...]})`, where <br>\n",
    "* `max_score` corresponds to the maximum TF-IDF score among all the query splits based on $Q$ and $DoE$.\n",
    "* The query split corresponding to the `max_score`, i.e., a python dictionary containing the tokens and entities list corresponding to the query split `{'tokens':[...], 'entities':[...]}`.\n",
    "\n",
    "### Running Time (Q1):\n",
    "* On CSE machines, your implementation for $\\textbf{parsing and indexing}$ approx 500 documents of average length of 500 tokens $\\textbf{SHOULD NOT take more than 120 seconds}$. \n",
    "* Once all the documents are indexed, $\\textbf{the query spliting and score}$ computation $\\textbf{SHOULD NOT take more than 15 sec}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How we test implementation of Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import project_part1 as project_part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'President Trump was on his way to new New York in New York City.',\n",
       " 2: 'New York Times mentioned an interesting story about Trump.',\n",
       " 3: 'I think it would be great if I can travel to New York this summer to see Trump.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = './Data/sample_documents.pickle'\n",
    "documents = pickle.load(open(fname,\"rb\"))\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step- 1. Construct the index...\n",
    "index = project_part1.InvertedIndex()\n",
    "\n",
    "index.index_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tokens': ['New', 'York', 'Times', 'Trump', 'travel'], 'entities': []}, {'tokens': ['Times', 'Trump', 'travel'], 'entities': ['New York']}, {'tokens': ['Trump', 'travel'], 'entities': ['New York Times']}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.562186043243266,\n",
       " {'tokens': ['Times', 'Trump', 'travel'], 'entities': ['New York']})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test cases\n",
    "Q = 'New York Times Trump travel'\n",
    "DoE = {'New York Times':0, 'New York':1,'New York City':2}\n",
    "doc_id = 3\n",
    "\n",
    "## 2. Split the query...\n",
    "query_splits = index.split_query(Q, DoE)\n",
    "\n",
    "\n",
    "## 3. Compute the max-score...\n",
    "result = index.max_score_query(query_splits, doc_id)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Submission and Feedback\n",
    "\n",
    "For project submission, you are required to submit the following files:\n",
    "\n",
    "1. Your implementation in a python file `project_part1.py`.\n",
    "\n",
    "2. A report `project_part1.pdf` You need to write a concise and simple report illustrating\n",
    "    - Implementation details of $Q1$.\n",
    "\n",
    "**Note:** Every student will be entitled to **15 Feedback Attempts** (use them wisely), we will use the last submission for final evaluation of **part-1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "以下是我的笔记："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from copy import deepcopy\n",
    "\n",
    "nlp = spacy.load('en') #load the English module\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        ## You should use these variable to store the term frequencies for tokens and entities...\n",
    "        self.tf_tokens\n",
    "        self.tf_entities\n",
    "\n",
    "        ## You should use these variable to store the inverse document frequencies for tokens and entities...\n",
    "        self.idf_tokens\n",
    "        self.idf_entities\n",
    "\n",
    "    ## Your implementation for indexing the documents...\n",
    "    def index_documents(self, documents):\n",
    "        \n",
    "        #1. construct the term frequency for tokens and entities\n",
    "        token_dict = {}\n",
    "        ent_dict = {}\n",
    "\n",
    "        for i in documents.keys():\n",
    "            doc = nlp(documents[i])\n",
    "            tokenlist, entlist = [], []\n",
    "\n",
    "            for token in doc:\n",
    "                if ((not token.is_stop) and (not token.is_punct)):\n",
    "                    tokenlist.append(token.text)\n",
    "            for ent in doc.ents:\n",
    "                entlist.append(ent.text)\n",
    "\n",
    "            tcount, ecount = Counter(tokenlist),Counter(entlist) #get the count of every token\n",
    "            single_word_ent = []\n",
    "            #aggregate entities into a dict, key:entity, value: {docID:count}\n",
    "            for key in ecount.keys(): \n",
    "                if(len(key.split()) == 1): #collect the single-word entity\n",
    "                    single_word_ent.append(key)\n",
    "                if key not in ent_dict:\n",
    "                    ent_dict[key] = {i: ecount[key]}\n",
    "                else:\n",
    "                    ent_dict[key][i] = ecount[key]\n",
    "\n",
    "            single_count = Counter(single_word_ent) \n",
    "\n",
    "            #aggregate tokens into a dict, key:token, value: {docID:count}\n",
    "            for key in tcount.keys():\n",
    "                if key in single_count.keys():  #exclude the single word entity in the token list\n",
    "                    tcount[key] -= single_count[key]\n",
    "\n",
    "                if tcount[key] != 0: \n",
    "                    if key not in token_dict:\n",
    "                        token_dict[key] = {i: tcount[key]}\n",
    "                    else:\n",
    "                        token_dict[key][i] = tcount[key]\n",
    "\n",
    "        #2. construct the tf and idf index\n",
    "        tf_tokens = deepcopy(token_dict) #use the same data format \n",
    "        tf_entities = deepcopy(ent_dict)\n",
    "        idf_tokens, idf_entities = {},{}\n",
    "        docs_num = len(documents)\n",
    "\n",
    "        # caluculate tf(norm) for tokens\n",
    "        for token in tf_tokens.keys():\n",
    "            for docid in tf_tokens[token].keys():\n",
    "                tf_tokens[token][docid] = 1.0 + log(1.0 + log(tf_tokens[token][docid]))\n",
    "\n",
    "        #calculate tf(norm) for entities\n",
    "        for ent in tf_entities.keys():\n",
    "            for docid in tf_entities[ent].keys():\n",
    "                tf_entities[ent][docid] = 1.0 + log(1.0 + log(tf_entities[ent][docid]))\n",
    "\n",
    "        # caluculate idf for tokens\n",
    "        for token in token_dict.keys():\n",
    "            idf_tokens[token] = log(docs_num / (1.0 + len(token_dict[token]))) + 1.0\n",
    "\n",
    "        # caluculate idf for entities\n",
    "        for ent in ent_dict.keys():\n",
    "            idf_entities[ent] = log(docs_num / (1.0 + len(ent_dict[ent]))) + 1.0\n",
    "        \n",
    "        self.tf_tokens = tf_tokens\n",
    "        self.tf_entities = tf_entities\n",
    "        self.idf_tokens = idf_tokens\n",
    "        self.idf_entities = idf_entities\n",
    "\n",
    "\n",
    "    ## Your implementation to split the query to tokens and entities...\n",
    "    def split_query(self, Q, DoE):\n",
    "        pass\n",
    "        return query_splits #a list\n",
    "\n",
    "\n",
    "\n",
    "    ## Your implementation to return the max score among all the query splits...\n",
    "    def max_score_query(self, query_splits, doc_id):\n",
    "        pass ## Replace this line with your implementation...\n",
    "        ## Output should be a tuple (max_score, {'tokens': [...], 'entities': [...]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'President Trump was on his way to new New York in New York City.',\n",
       " 2: 'New York Times mentioned an interesting story about Trump.',\n",
       " 3: 'I think it would be great if I can travel to New York this summer to see Trump.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tokens_score': 1.4054651081081644, 'entities_score': 1.0, 'combined_score': 1.562186043243266\n"
     ]
    }
   ],
   "source": [
    "#part 3\n",
    "query =   {'tokens': ['Times', 'Trump', 'travel'], 'entities': ['New York']}\n",
    "\n",
    "s,s1,s2 = 0.0, 0.0, 0.0\n",
    "docid = 3\n",
    "for ent in query['entities']:\n",
    "    if (ent in tf_entities) and (docid  in tf_entities[ent]): # if the token appear in this doc\n",
    "        s1 += tf_entities[ent][docid] * idf_entities[ent] \n",
    "for token in query['tokens']:\n",
    "    if (token in tf_tokens) and (docid  in tf_tokens[token]): # if the token appear in this doc\n",
    "        s2 += tf_tokens[token][docid] * idf_tokens[token]  \n",
    "s = s1+0.4*s2\n",
    "print(f'\\'tokens_score\\': {s2}, \\'entities_score\\': {s1}, \\'combined_score\\': {s1+0.4*s2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-50e24ec7c70e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m## 2. Split the query...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mquery_splits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "Q = 'New York Times Trump travel'\n",
    "DoE = {'New York Times':0, 'New York':1,'New York City':2}\n",
    "doc_id = 3\n",
    "\n",
    "## 2. Split the query...\n",
    "query_splits = index.split_query(Q, DoE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def split_query(Q, DoE):\n",
    "    query_splits = []\n",
    "    \n",
    "    #1.select the probable entities\n",
    "    '''1)a empty entity list\n",
    "       2)find all combinations as eneities of the query\n",
    "       3)if a entity is in DOE, append it in the entity list\n",
    "    '''\n",
    "    tokenlist, entitylist = [], []\n",
    "    query = nlp(Q)\n",
    "    #1.1 find the single-word entities and the tokens\n",
    "    for token in query:\n",
    "        tokenlist.append(token.text)\n",
    "        if (token in DoE):\n",
    "            entitylist.append(token.text) #this is the single-word entity\n",
    "    print('query_tokens: ', tokenlist)\n",
    "    query_splits.append({'tokens': tokenlist, 'entities': []})\n",
    "    \n",
    "    #1.2 construct combinations of tokens and find multi-word eentities\n",
    "    for i in range(2, len(tokenlist)+1): #from length = 2 to len(tokenlist)\n",
    "        for combi in combinations(tokenlist, i): #get the index combination\n",
    "            ent = ''\n",
    "            for i in range(len(combi)):\n",
    "                if (i != (len(combi)-1)):\n",
    "                    ent += combi[i] + ' '\n",
    "                else:\n",
    "                    ent += combi[i]\n",
    "            if (ent in DoE):\n",
    "                entitylist.append(ent) #this is the multi-word entity\n",
    "    print('step1: ', entitylist)\n",
    "    \n",
    "    #2.enumerate all possible subsets of entities, using combinations\n",
    "    '''\n",
    "    2.1 find the combinations\n",
    "    2.2 check token count of each combination doesn't exceed Q_count\n",
    "    '''\n",
    "    entity_subset = []\n",
    "    Q_count = Counter(tokenlist) #token count in Query\n",
    "    print ('qurey counter: ', Q_count)\n",
    "    #2.1 find the combinations\n",
    "    for subset_len in range(1, len(entitylist)+1):\n",
    "        for subset in combinations(entitylist, subset_len):\n",
    "            print(subset)\n",
    "            #2.2.1 count the token in subset\n",
    "            subset_word = []\n",
    "            for j in range(subset_len): #split the words in each entity of the subset\n",
    "                subset_word += subset[j].split(' ')\n",
    "            subset_counter = Counter(subset_word)\n",
    "            print(subset_counter)\n",
    "            #2.2.2 check that if the count of subset exceed Q-count\n",
    "            exceed = 0\n",
    "            for key in subset_counter.keys():\n",
    "                if subset_counter[key] > Q_count[key]:\n",
    "                    exceed = 1\n",
    "                    break\n",
    "                    \n",
    "            if exceed == 0:\n",
    "                #3 for filtered entity subset,append it to the query_splits list\n",
    "                token_counter = Q_count - subset_counter\n",
    "                query_splits.append({'tokens': list(token_counter.elements()), 'entities': list(subset)})\n",
    "                entity_subset.append(subset)\n",
    "    print('entity subset: ', entity_subset)\n",
    "    print()\n",
    "    print(query_splits)\n",
    "    return query_splits\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_tokens:  ['New', 'York', 'Times', 'Trump', 'travel']\n",
      "step1:  ['New York', 'New York Times']\n",
      "qurey counter:  Counter({'New': 1, 'York': 1, 'Times': 1, 'Trump': 1, 'travel': 1})\n",
      "('New York',)\n",
      "Counter({'New': 1, 'York': 1})\n",
      "('New York Times',)\n",
      "Counter({'New': 1, 'York': 1, 'Times': 1})\n",
      "('New York', 'New York Times')\n",
      "Counter({'New': 2, 'York': 2, 'Times': 1})\n",
      "entity subset:  [('New York',), ('New York Times',)]\n",
      "\n",
      "[{'tokens': ['New', 'York', 'Times', 'Trump', 'travel'], 'entities': []}, {'tokens': ['Times', 'Trump', 'travel'], 'entities': ['New York']}, {'tokens': ['Trump', 'travel'], 'entities': ['New York Times']}]\n"
     ]
    }
   ],
   "source": [
    "Q = 'New York Times Trump travel'\n",
    "DoE = {'New York Times':0, 'New York':1,'New York City':2}\n",
    "query_splits = split_query(Q, DoE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_score_query(query_splits, doc_id):\n",
    "    score_list = []\n",
    "    \n",
    "    for query in query_splits:\n",
    "        score,score1,score2 = 0.0, 0.0, 0.0\n",
    "        for ent in query['entities']:\n",
    "            if (ent in tf_entities) and (docid  in tf_entities[ent]): # if the token appear in this doc\n",
    "                score1 += tf_entities[ent][docid] * idf_entities[ent] \n",
    "        for token in query['tokens']:\n",
    "            if (token in tf_tokens) and (docid  in tf_tokens[token]): # if the token appear in this doc\n",
    "                score2 += tf_tokens[token][docid] * idf_tokens[token]  \n",
    "        score = score1 + 0.4*score2\n",
    "        score_list.append(score)\n",
    "        print(f'\\'tokens_score\\': {score2}, \\'entities_score\\': {score1}, \\'combined_score\\': {score}')\n",
    "    print(score_list)\n",
    "    \n",
    "    max_score = max(score_list)\n",
    "    return (max_score, query_splits[score_list.index(max_score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tokens_score': 2.8301009632046026, 'entities_score': 0.0, 'combined_score': 1.132040385281841\n",
      "'tokens_score': 1.4054651081081644, 'entities_score': 1.0, 'combined_score': 1.562186043243266\n",
      "'tokens_score': 1.4054651081081644, 'entities_score': 0.0, 'combined_score': 0.5621860432432658\n",
      "[1.132040385281841, 1.562186043243266, 0.5621860432432658]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.562186043243266,\n",
       " {'tokens': ['Times', 'Trump', 'travel'], 'entities': ['New York']})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_id = 3\n",
    "result = max_score_query(query_splits, doc_id)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'New York': 1, 'New York Times': 2})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c - Counter(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nelonelo\n"
     ]
    }
   ],
   "source": [
    "print('nelo' * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Parent():\n",
    "    def __init__(self):\n",
    "        self.name = ''\n",
    "        \n",
    "tom = Parent()\n",
    "tom.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        ## You should use these variable to store the term frequencies for tokens and entities...\n",
    "        self.tf_tokens = []\n",
    "        self.tf_entities = [] \n",
    "\n",
    "        ## You should use these variable to store the inverse document frequencies for tokens and entities...\n",
    "        self.idf_tokens = []\n",
    "        self.idf_entities = []\n",
    "\n",
    "index = InvertedIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.tf_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is me!\n"
     ]
    }
   ],
   "source": [
    "import project_part1 as project_part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "module"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(project_part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<project_part1.InvertedIndex at 0x17ef1fffda0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_part1.InvertedIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
